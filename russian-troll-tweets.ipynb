{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17897,"sourceType":"datasetVersion","datasetId":13184}],"dockerImageVersionId":9575,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Russian Troll Tweets Analysis\n\nI am excited to apply my recent learnings in text analytics to delve into tweet data, which marks my first venture into working with text data.\n\nMy considerations for analysis encompass the following points:\n\n1. **Identifying Trending Hashtags:** Exploring the dataset to determine the most prevalent and trending hashtags.\n2. **Temporal Tweet Analysis:** Assessing tweet frequency to ascertain any notable spikes corresponding to major events. Intuitively, an uptick in tweets is anticipated during significant occurrences.\n3. **User Mentions Network:** Constructing a graph to visualize user interactions by mapping out the most frequently mentioned users.\n4. **Clustering Users by Topics:** Investigating the feasibility of clustering users based on thematic similarities evident in their tweets.\n5. **User Prediction from Tweet Content:** Exploring the potential to predict users based on their tweet contents using predictive modeling techniques.\n\nThis endeavor into text analytics aims to uncover nuanced insights within tweet data, presenting opportunities for comprehensive analysis and predictive modeling.\n","metadata":{"_uuid":"61fcf0e527e693b21434f71a6322de3b34857c65"}},{"cell_type":"code","source":"# Basic library loading\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n%pylab inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets take a look at the first few rows of our data!","metadata":{"_uuid":"a7af5f5213921d49adc58b41610e806f41b3f27a"}},{"cell_type":"code","source":"# Read in the dataset required\ntroll = pd.read_csv('../input/tweets.csv')\nprint(troll.shape)\ntroll.head(2)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset comprises 203,482 rows and 16 columns, containing vital information such as the userid, tweetid, tweet text, hashtags, mentions, along with metrics indicating the tweet's engagement through retweets and favorites. Additionally, it incorporates a datetime column denoted as created_str.\n\nOur next course of action involves evaluating the presence of missing values within the dataframe.","metadata":{"_uuid":"6a98b9c875fb5476a5edac85ba86ded259523360"}},{"cell_type":"code","source":"troll.isnull().sum().sort_values(ascending = False)","metadata":{"_uuid":"fae1ef42dce231289fe00d7d9a088cc05d00a856","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approximately 21 missing values have been identified within the text column, a pivotal component for our analysis. Consequently, we will proceed by eliminating rows containing NaN values specifically in the text column, ensuring a more robust dataset for our analytical endeavors.","metadata":{"_uuid":"505eca9de13558d52e8a342feb09efdf6072a89b"}},{"cell_type":"code","source":"# drop NAs in the text column and update the troll dataframe\ntroll.dropna(subset = ['text'], inplace = True)","metadata":{"_uuid":"61a2f00631a4fcb64c5f6971b8331a57ebe7d49a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nWe'll inspect the data types of the columns and convert the created_str column to datetime if it's not already in that format.","metadata":{"_uuid":"411ef37f5243c92c18f5ea6fe5b7c8fadb8205bb"}},{"cell_type":"code","source":"print(troll.dtypes)","metadata":{"_uuid":"5d8380dc20e2334299d27778ddc3542bf180a43d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nHence:\n\n1. user_id, tweet_id, retweeted_status_id, and in_reply_to_status_id have been interpreted as float data types, although they should be of object data type. These columns are not intended for computations.\n2. Additionally, the created_str column has been imported as an object data type; however, for subsequent processing, it is imperative to convert it to datetime format.","metadata":{"_uuid":"d1a376a06ca96f0d02357be417e7df107538997b"}},{"cell_type":"code","source":"# convert created_str to datetime format\ntroll['created_str'] = pd.to_datetime(troll['created_str'])\n\n# convert ids to object datatype\ncolumns = ['user_id', 'tweet_id', 'retweeted_status_id', \n           'retweeted_status_id', 'in_reply_to_status_id']\n\nfor column in columns:\n    troll[column] = troll[column].astype('object')","metadata":{"_uuid":"6bc195a3ab9aed947f185e909e2e069b2981c42d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check data types of output dataframe","metadata":{"_uuid":"d4d0e19e7a4542256d4a6987ae2a9b16431cf02c"}},{"cell_type":"code","source":"troll.dtypes","metadata":{"_uuid":"0c770a5f7eec4a7184433f02dd5e83934f65d300","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What period of tweets does this data capture?","metadata":{"_uuid":"57a4ee2072b2e4ee48ce27756b70e19db4509bab"}},{"cell_type":"code","source":"start_date_tweet = troll['created_str'].min()\nend_date_tweet = troll['created_str'].max()\n\nprint(start_date_tweet, end_date_tweet)","metadata":{"_uuid":"ce32f52b594b9ce25bf3541dbe03dbd5346d24d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We possess a corpus of tweets spanning approximately three years, commencing from July 14, 2014, and culminating on September 26, 2017. These tweets are timestamped with both date and time details. To facilitate our analysis, we intend to generate a new column exclusively dedicated to storing the date component extracted from these timestamps.","metadata":{"_uuid":"203aed5c0095702686bd997170ceb757a42ce3f7"}},{"cell_type":"code","source":"# created_str_data holds the date component of the created_str column\ntroll['created_str_date'] = pd.to_datetime(troll['created_str'].dt.date)","metadata":{"_uuid":"2e1974959a3d529491b3d922874d38b7028612f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Tweet Trends Over Time\nGiven that each row represents a single tweet recorded on a specific date (created_str_date), our approach involves tallying the occurrences of each date. By doing so, we can derive the frequency of tweets per day, providing valuable insights into the temporal distribution and trend of these tweets over the aforementioned timeline.","metadata":{"_uuid":"ca88cb8b6558ad3b124d357cf47f6c2b4d26a42e"}},{"cell_type":"code","source":"# Count the number of times a date appears in the dataset and convert to dataframe\ntweet_trend = pd.DataFrame(troll['created_str_date'].value_counts())\n\n# index is date, columns indicate tweet count on that day\ntweet_trend.columns = ['tweet_count']\n\n# sort the dataframe by the dates to have them in order\ntweet_trend.sort_index(ascending = True, inplace = True)","metadata":{"_uuid":"5d2268992965e7e7e4cb3c67630bd4fd264bfc38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a line plot of the tweet count data and give some pretty labels! ;)\n# the 'rot' argument control x-axis ticks rotation\nplt.style.use('seaborn-darkgrid')\ntweet_trend['tweet_count'].plot(linestyle = \"-\", figsize = (12,8), rot = 45, color = 'k',\n                               linewidth = 1)\nplt.title('Tweet counts by date', fontsize = 15)\nplt.xlabel('Date', fontsize = 13)\nplt.ylabel('Tweet Count', fontsize = 13)","metadata":{"_uuid":"0dc022e8058617e964790eefb59c6a1015d80ffb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To ascertain the potential impact of these tweets on the US presidential elections, particularly during critical junctures such as Trump's rallies or significant milestones within his campaign, we'll acquire the dates associated with these events. Subsequently, we'll endeavor to overlay this event data onto our line plot to explore potential correlations or heightened tweet activity during these pivotal moments.'**\n\n[Here is the link to the important dates in Trump's presidential campaign](https://www.reuters.com/article/us-usa-election-timeline-factbox/timeline-pivotal-moments-in-trumps-presidential-campaign-idUSKBN1341FJ)","metadata":{"_uuid":"773e91ea0786604961103dca7ed9a8cdea07b03a"}},{"cell_type":"code","source":"# these are dates corresponding to important dates from the trump campaign.\ndates_list = ['2015-06-16', '2015-12-07', '2016-02-01',\n              '2016-03-01', '2016-03-03', '2016-03-11',\n              '2016-05-03', '2016-05-26', '2016-06-20', \n              '2016-07-15', '2016-07-21', '2016-08-17',\n              '2016-09-01', '2016-10-07', '2016-11-08']\n\n# create a series of these dates.\nimportant_dates = pd.Series(pd.to_datetime(dates_list))\n\n# add columns to identify important events, and mark a 0 or 1.\ntweet_trend['Important Events'] = False\ntweet_trend.loc[important_dates, 'Important Events'] = True\ntweet_trend['values'] = 0\ntweet_trend.loc[important_dates, 'values'] = 1","metadata":{"_uuid":"72bb05e9a2a199678585603170d9e8aa7f8c62ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the line chart for trend, a monthly average of tweet counts and add red dots to \n# mark important events.\nplt.style.use('seaborn-darkgrid')\ntweet_trend['tweet_count'].plot(linestyle = \"--\", \n                                figsize = (12,8), rot = 45, \n                                color = 'k',\n                                label = 'Tweet Count per Day',\n                               linewidth = 1)\n\n# plot dots for where values in the tweet_trend df are 1\nplt.plot(tweet_trend[tweet_trend['Important Events'] == True].index.values,\n         tweet_trend.loc[tweet_trend['Important Events'] == True, 'values'],\n         marker = 'o', \n         color = 'r',\n         linestyle = 'none',\n        label = 'Important Dates in campaign')\n\n# Lets add a 30 day moving average on top to view the trend! Min_periods tells rolling() to\n# use 10 points if 30 not available!\nplt.plot(tweet_trend['tweet_count'].rolling(window = 30, min_periods = 10).mean(), \n         color = 'r', \n         label = '30 Day Moving Avg # of tweets')\nplt.title('Tweet counts by date', fontsize = 15)\nplt.xlabel('Date', fontsize = 13)\nplt.ylabel('Tweet Count', fontsize = 13)\nplt.legend(loc = 'best')","metadata":{"_uuid":"37897488520aead8e3473419075943ed628bfc63","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, the election of the US president occurred on November 8, 2016, which is denoted as the final red dot on the chart. Noticeably, a considerable surge in tweet activity is observed towards the culmination of his campaign, indicating heightened engagement during this period.\n\nCalculating the percentage change in tweet counts between consecutive dates will enable us to discern fluctuations and ascertain if significant dates correlate with notable spikes in tweet activity. This analysis could potentially unveil any direct relationships between pivotal moments and increased tweet engagement.","metadata":{"_uuid":"980a1f0d81840d5e7239054406cac431ee0dbfa1"}},{"cell_type":"code","source":"# Calculate the percentage change in tweet counts\ntweet_trend['Pct_Chg_tweets'] = tweet_trend['tweet_count'].pct_change()*100\n\n# Lets see values only for the important dates. This Pct_Chg_tweets shows us the percentage\n# change in tweets for the day of the event versus the previous day!\ntweet_trend.loc[tweet_trend['values'] == 1,['tweet_count', 'Pct_Chg_tweets']]","metadata":{"_uuid":"2556a2fc54d8aeae451fe6c40466a79e5de007eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the percentage change in tweet counts against the associated dates along the x-axis will enable us to visualize any potential correlations between these pivotal event dates and the fluctuations in tweet engagement. This representation will provide insights into the relationship between significant events and the subsequent fluctuations in tweet activity, whether positive or negative","metadata":{"_uuid":"1c6e3e2e9ac31b6035781f9dddb552285c838c7d"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('seaborn-darkgrid')\n\n# line plot of the percentage change in tweet counts\ntweet_trend['Pct_Chg_tweets'].plot(linestyle = \"--\", figsize = (12,8), rot = 45, \n                                   color = 'k',\n                                  linewidth = 1)\n# add the dots for important events!\nplt.plot(tweet_trend[tweet_trend['Important Events'] == True].index.values,\n         tweet_trend.loc[tweet_trend['Important Events'] == True, 'values'],\n         marker = 'o', \n         color = 'r',\n         linestyle = 'none')\nplt.title('Tweet count change', fontsize = 15)\nplt.xlabel('Date', fontsize = 13)\nplt.ylabel('Tweet Count Change', fontsize = 13)","metadata":{"_uuid":"1be209cc838423cb8b93f56afc0f029b8b546145","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Analytics\nWe've observed that within the tweets, there exist supplementary components such as RT mentions, links, and hashtags. To streamline our analysis, it is imperative to segregate these elements, allowing for a focused examination of either the tweet text exclusively or an isolated study of the hashtags, based on our analytical preferences.","metadata":{"_uuid":"16f8020204a96ced92b52764ebf435c9b5f7b455","trusted":true}},{"cell_type":"code","source":"# take a look at what the 'text' column holds\ntroll['text'].head(10)","metadata":{"_uuid":"02e7417a0fea32dc657e7911da7cd6060a00197f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above shows that  - <br>\n1. **Retweets** begin with the **keyword 'RT'**. These are followed by _@userkey_.\n2. **Hashtags** begin with a **_#_** and are one continuous string with a space next to them!\n3. **Links** begin with **_https://_ or _http://_** and can be present anywhere in the string.\n4. **There can be multiple links and hashtags in a tweet, but retweet identifier is just one.**\n5. **User mentions** begin with **'@'** and are a continuous word!","metadata":{"_uuid":"d4f1c12c0eb5b2c9152b9362106575c7f543d494"}},{"cell_type":"markdown","source":"**First let's remove the RT mentions from tweets.**","metadata":{"_uuid":"c565da16540072d4c9f7ff87e6167e57cfc908ff"}},{"cell_type":"code","source":"# define a function that takes in a tweet and throws out the text without the RT.\ndef remove_retweet(tweet):\n    '''Given a tweet, remove the retweet element from it'''\n    text_only = []\n    if len(re.findall(\"^RT.*?:(.*)\", tweet)) > 0:\n        text_only.append(re.findall(\"^RT.*?:(.*)\", tweet)[0])\n    else:\n        text_only.append(tweet)\n    return text_only[0]\n\n# extract texts and place in a list\ntext_only = troll['text'].map(remove_retweet)","metadata":{"_uuid":"29d0577c453378ea7fea692173fbaa37d7a04b67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next, let's remove the links from these tweet texts so that we have a column for just the tweet text devoid of any links and RT mentions.**","metadata":{"_uuid":"ac07957e1b563c97ba4a88a7afd125ba41fa105f"}},{"cell_type":"code","source":"# this method checks for links and removes these from the tweet provided!\ndef remove_links(tweet):\n    '''Provide a tweet and remove the links from it'''\n    text_only = []\n    if len(re.findall(\"(https://[^\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(https://[^\\s]+)\", \"\", tweet)\n    if len(re.findall(\"(http://[^\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(http://[^\\s]+)\", \"\", tweet)    \n    text_only.append(tweet)\n    return text_only[0]\n\ntext_no_links = text_only.map(remove_links)","metadata":{"_uuid":"5f22f442c602a5e1d89e00905a69709ddc684ac9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets remove hashtags from these!**","metadata":{"_uuid":"58e564e1c59ee8d1800c0fdd3617945e4ef5976a"}},{"cell_type":"code","source":"def remove_hashtags(tweet):\n    '''Provide a tweet and remove hashtags from it'''\n    hashtags_only = []\n    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(#[^#\\s]+)\", \"\", tweet) \n    hashtags_only.append(tweet)\n    return hashtags_only[0]\n\ntext_all_removed = text_no_links.map(remove_hashtags)","metadata":{"_uuid":"dfeff9f54086490edb9bfbd23d54f4870364ef74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's remove all extra spaces from the beginning and the end of the text that we got!**","metadata":{"_uuid":"b60116f3045a40da64cee2765a47a81918eef430"}},{"cell_type":"code","source":"def remove_extraneous(tweet):\n    '''Given a text, remove unnecessary characters from the beginning and the end'''\n    tweet = tweet.rstrip()\n    tweet = tweet.lstrip()\n    tweet = tweet.rstrip(\")\")\n    tweet = tweet.lstrip(\"(\")\n    tweet = re.sub(\"\\.\", \"\", tweet)\n    return tweet\n\ntext_clean = text_all_removed.map(remove_extraneous)","metadata":{"_uuid":"21beed242373d8e8bfe9a29bc3bdce401c75b7bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**At last, we remove the user mentions from the text_clean** But before we do that, lets store these user mentions in a new column within the dataframe!","metadata":{"_uuid":"7c2c958012a4d16d488b4ab68161150d084627aa"}},{"cell_type":"code","source":"# in case no mention present, we return \"0\"\ndef extract_mentions(tweet):\n    '''Given a tweet, this function returns the user mentions'''\n    mentions = []\n    if len(re.findall('@[^\\s@]+', tweet))>0:\n        mentions.append(re.findall('@([^\\s@]+)', tweet))\n    else:\n        mentions.append([\"0\"])\n    return mentions[0]\n\n# Put the user mentions in a new column in our dataframe\ntroll['user_mentions'] = text_clean.map(extract_mentions)","metadata":{"_uuid":"d52e1ef64ef647b598e6624488fb938da01a9d6f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now lets remove the mentions from the tweet text\ndef remove_mentions(tweet):\n    '''Given a text, remove the user mentions'''\n    mentions = []\n    if len(re.findall('@[^\\s@]+', tweet))>0:\n        tweet = re.sub('@[^\\s@]+', \"\" , tweet)\n        mentions.append(tweet)\n    else:\n        mentions.append(tweet)\n    return mentions[0]\n\ntext_clean_final = text_clean.map(remove_mentions)","metadata":{"_uuid":"0bec9d1ecdd692bbd91fd3dcbc398c7d0a7a0cfe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the cleaned tweet text now available to us in the _Text_clean_ list, let's append it to our dataframe!","metadata":{"_uuid":"c4de77bf563468acc3551e69af4f7e4767a7715a"}},{"cell_type":"code","source":"troll['tweet_text_only'] = text_clean_final","metadata":{"_uuid":"a4d3c09846ca02fd5a0fd09b60fcd77bb2b84370","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What hashtags are being used the most?\nFirst, lets use the function created above but use it in a way that we can extract the hashtags and not remove them!","metadata":{"_uuid":"b3293693b3a89e2f39a7d58a447eb608f6d42277"}},{"cell_type":"code","source":"# in case hashtags are not found, we will use \"0\" as the placeholder\ndef extract_hashtags(tweet):\n    '''Provide a tweet and extract hashtags from it'''\n    hashtags_only = []\n    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n        hashtags_only.append(re.findall(\"(#[^#\\s]+)\", tweet))\n    else:\n        hashtags_only.append([\"0\"])\n    return hashtags_only[0]\n\n# make a new column to store the extracted hashtags and view them!\ntroll['tweet_hashtags'] = troll['text'].map(extract_hashtags)\ntroll['tweet_hashtags'].head(10)","metadata":{"_uuid":"b33eaae4e9ce0421e437a67792c86c10273bc633","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nect we need to extract all unique hashtags from the hashtags column just created and take a value count on those!","metadata":{"_uuid":"3315d00e85fbe1cfd9e8dc6ae7f19af8a0900864"}},{"cell_type":"code","source":"# create a list of all hashtags\nall_hashtags = troll['tweet_hashtags'].tolist()\n\n# Next we observe that our all_hashtags is a list of lists...lets change that\ncleaned_hashtags = []\nfor i in all_hashtags:\n    for j in i:\n            cleaned_hashtags.append(j)\n\n# Convert cleaned_hashtags to a series and count the most frequent occuring\ncleaned_hashtag_series = pd.Series(cleaned_hashtags)\nhashtag_counts = cleaned_hashtag_series.value_counts()","metadata":{"_uuid":"d7ed3b548d4bcdfad81029f87a360700d7dbfbe7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll initiate by generating a wordcloud showcasing the most prevalent hashtags. To accomplish this, we'll compile the word list as a unified string, delineated by spaces. Subsequently, employing an algorithm, we'll tally the frequency of each term, visualizing the words in larger or bolder fonts proportional to their frequency within the dataset.","metadata":{"_uuid":"5cfdb020114a0b286060c5ef5f280105200fd544"}},{"cell_type":"code","source":"# Get hashtag terms from the series and convert to list\nhashes = cleaned_hashtag_series.values\nhashes = hashes.tolist()\n\n# convert list to one string with all the words\nhashes_words = \" \".join(hashes)\n\n# generate the wordcloud. the max_words argument controls the number of words on the cloud\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width= 1600, height = 800, \n                      relative_scaling = 1.0, \n                      colormap = \"Blues\",\n                     max_words = 100).generate(hashes_words)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"_uuid":"0d03b6bb4a8f644c0c33043b2e1bcaa711a6afc3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\nplt.figure(figsize = (12,8))\nplt.barh(y = hashtag_counts[1:21].index.values, width = hashtag_counts[1:21])\nplt.title(\"Top 20 Hashtags used in Troll tweets\", fontsize = 15)\nplt.xlabel('Count of hashtags', fontsize = 13)\nplt.ylabel('Hashtags', fontsize = 13)","metadata":{"_uuid":"d38ea748f1aeaec5ce6e4126587c9fe20938ec2a","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon analyzing the most frequently used hashtags, a noteworthy trend emerges:\n\n1. **#TCOT**\n2. **#POLITICS**\n3. **#PJNET**\n4. **#MAGA**\n5. **#Trump**\n\nThese hashtags predominantly signify support for the current president's campaign, indicating a prevalent inclination of the trolls toward endorsing his candidacy.\n\nWithin the top 20 most utilized hashtags, it's discernible that **#WAKEUPAMERICA**, **#P2**, and **@HILLARY** demonstrate opposition to the candidacy. However, their frequency of use is considerably lower compared to the aforementioned pro-campaign hashtags.\n\nOf particular interest is **#2A**, referencing the Second Amendment of the US constitution, aligning with the president's stance on the right to keep and bear arms.\n\nSurprisingly, the trolls also utilized the hashtag **#MERKELMUSSBLEIBEN**, advocating support for German Chancellor Angela Merkel, translating to 'Merkel Must Stay'.\n","metadata":{"_uuid":"e4640ac88c80f3f504d3d7bec8689e9b902e3e5d"}},{"cell_type":"markdown","source":"## Were these hashtags used most just before the presidents campaign?\n\nTo analyze this, we will use the top 6 hashtags and a count of how many times these were used on the dates provided in the *created_str_date*. \n\nLet's first extract the dates and the hashtags used on those dates. We would then count each top hashtag in these and proceed.","metadata":{"_uuid":"b6a3cfff0bdfa75bd4a3838e3f89520cbaacc28d"}},{"cell_type":"code","source":"# Create a dataframe with just the date and the hashtags in the tweet on that date\nhashtag_date_df = troll[['created_str_date', 'tweet_hashtags']]\nhashtag_date_df = hashtag_date_df.reset_index(drop = True)\n\n# extract a list of hashtags from the dataframe\nall_hashtags = hashtag_date_df['tweet_hashtags'].tolist()\n\nhashtag_date_df.head()","metadata":{"_uuid":"1d80bb0f166aacbfe1b6b2efe0078a47f365ab3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For the top 6 hashtags, lets calculate how many times that appears against each date!\ncount_dict = {}\nfor i in hashtag_counts.index.values[1:7]:\n    count_hash = []\n    for j in all_hashtags:\n        count_hash.append(j.count(i))\n    count_dict[i] = count_hash","metadata":{"_uuid":"a67ade6b717a7d17ee265c6f3630a97d6649a856","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dataframe from the hashtags\nhashtag_count_df = pd.DataFrame(count_dict)\n\n# concatenate this dataframe with the hashtag_count_df\nhashtag_count_df = pd.concat([hashtag_date_df, hashtag_count_df], axis = 1)","metadata":{"_uuid":"cb75b3cfd0c030b49a3e09513e3bcf908c606e5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hashtag_count_df.head()","metadata":{"_uuid":"90b513bc8e0f753188b208aaf2bdfdbe5abac83e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each column above is a hashtag and each cell underneath tells us the count of times that hashtag appeared in a tweet on that date! **We now need to summarise this data at the monthly level to get a sense of the month-on-month usage of these hashtags!**","metadata":{"_uuid":"0ed69fd52888b0dceac6d5b05a6908e42f57d1d0"}},{"cell_type":"code","source":"# change the created_str column into datetime format and extract just the date from it\nhashtag_count_df['created_str_date'] = pd.to_datetime(hashtag_count_df['created_str_date'])\n\n# set the index so as to plot the time series\nhashtag_count_df.set_index('created_str_date', inplace = True)\n\n# get a monthly sum of the tweets for each of these hashtags\nhashtag_count_df_pivot = hashtag_count_df.resample('M').sum()\n\n# replace 0 with nan so that these can be removed in rows where they are all NaNs\nhashtag_count_df_pivot.replace(0, np.nan, inplace = True)\nhashtag_count_df_pivot.dropna(how = 'all', inplace = True, axis = 0)\n\n# replace NaNs back by 0s so that we can plot\nhashtag_count_df_pivot.replace(np.nan, 0, inplace = True)\nhashtag_count_df_pivot","metadata":{"_uuid":"d06a12034487e046c97aa2592ee1b16bb8c23188","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\n# create a 3 by 2 subplot to hold the trend of all hashtags\nfigure, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = subplots(nrows = 3,\n                                                       ncols = 2,\n                                                       sharey = True,\n                                                       figsize = (10,8))\n\nplt.subplots_adjust(top = 1, hspace = 0.9)\nhashtag_count_df_pivot['#politics'].plot(linestyle = \"-\", marker = \"o\", color = \"green\",ax = ax1)\nax1.set_title(\"#POLITICS\", fontsize = 10)\nax1.set_xlabel('Date', fontsize = 12)\n\nhashtag_count_df_pivot['#tcot'].plot(linestyle = \"-\", marker = \"o\", color = \"red\", ax = ax2)\nax2.set_title(\"#TCOT\", fontsize = 10)\nax2.set_xlabel('Date', fontsize = 12)\n\nhashtag_count_df_pivot['#MAGA'].plot(linestyle = \"-\", marker = \"o\", color = \"orange\", ax = ax3)\nax3.set_title(\"#MAGA\", fontsize = 10)\nax3.set_xlabel('Date', fontsize = 12)\n\nhashtag_count_df_pivot['#PJNET'].plot(linestyle = \"-\", marker = \"o\", color = \"blue\",ax = ax4)\nax4.set_title(\"#PJNET\", fontsize = 10)\nax4.set_xlabel('Date', fontsize = 12)\n\nhashtag_count_df_pivot['#news'].plot(linestyle = \"-\", marker = \"o\", color = \"grey\", ax = ax5)\nax5.set_title(\"#NEWS\", fontsize = 10)\nax5.set_xlabel('Date', fontsize = 12)\n\nhashtag_count_df_pivot['#Trump'].plot(linestyle = \"-\", marker = \"o\", color = \"maroon\", ax = ax6)\nax6.set_title(\"#TRUMP\", fontsize = 10)\nax6.set_xlabel('Date', fontsize = 12)","metadata":{"_uuid":"a705c7bb5d3633ac6749293d849ddbf68710d4ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We see that most of these hashtags picked up in the year 2016 near March or later in July, close to the elections! This is also the time when alleged interference by Russian trolls started!** \n\nWe see the largest peak in the **#politics**.  Maybe tweeting about politics got these trolls the largest following!","metadata":{"_uuid":"d149c8576f99e82508bf11f10518c2540f057e88"}},{"cell_type":"markdown","source":"## Lets look at user mentions!\nWe already have the user mentions in a column in our **troll** dataframe! Let's create a dataframe with each user's tweet with the user mentions against it! ","metadata":{"_uuid":"91b62a5b1ed429b41acd4c577111da18342ab84f","trusted":true}},{"cell_type":"code","source":"troll['user_mentions'].head(10)","metadata":{"_uuid":"d6462d8dc8c3dc2698a07d9f6495ecd8dcd088d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now separate out the *user_key* and *user_mentions* columns from this dataframe!","metadata":{"_uuid":"69ff8bfe4c0260e0b5559b157f62f1896235b311"}},{"cell_type":"code","source":"user_mention = troll.loc[:, ['user_key', 'user_mentions']]\nuser_mention.head(6)","metadata":{"_uuid":"167f51a245fd42d676b67d0fc6b4e5286a0400b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove rows where no user is mentioned! These are rows where the *user_mentions* column has a [0].","metadata":{"_uuid":"f03e3834e81991e6bb58c57978a1debd4fa862db"}},{"cell_type":"code","source":"row_remove_mask = user_mention['user_mentions'].map(lambda x: \"0\" in x)","metadata":{"_uuid":"aced9968ddc3b85168543ed035342e21ea728c2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us a T/F series - **True** where *user_mentions* are empty, **False** otherwise","metadata":{"_uuid":"8c17872fdcac975f1bc92801d1843e1e1a389773"}},{"cell_type":"code","source":"np.sum(row_remove_mask)","metadata":{"_uuid":"4d3342fb52cf33590b3e886f61a2b7ebe7b34d44","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep rows where row_remove_mask is FALSE\nuser_mention_df = user_mention.loc[~row_remove_mask, :]\nuser_mention_df.reset_index(drop = True, inplace = True)\nuser_mention_df.head(10)","metadata":{"_uuid":"b0fb88e3a576271b2f00bbad487b9dba9cdf59ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for each row, create a one-to-one tuple of user and his user mention\nnew_list = []\nfor i in range(len(user_mention_df)):\n    for j in user_mention_df.loc[i, \"user_mentions\"]:\n        (a,b) = (user_mention_df.loc[i, 'user_key'], j)\n        new_list.append((a,b))","metadata":{"_uuid":"87560ee5bcc635ac802dbb5318cf22778da84521","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a tuple with each user -> user_mention value. Lets get a dataframe from this!","metadata":{"_uuid":"b725ecb0dda92ad381963894b4b73ebd4159047c"}},{"cell_type":"code","source":"user_mention_clean_df = pd.DataFrame({\"User_Key\": [a for (a,b) in new_list],\n                                     \"User_Mention\": [b for (a,b) in new_list]})\nuser_mention_clean_df.head()","metadata":{"_uuid":"18a34ceab440449f60ca1f1774b6eb234869dbe6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hashtags based clustering\nCan we get some cluster of users who use similar hashtags? First we will create a dataframe which holds the *user_key* and the *tweet_hashtags*.","metadata":{"_uuid":"a57d46871630865b7d425a16d4b7ff7425965966"}},{"cell_type":"code","source":"# create a df with user and hashtags in one tweet\nuser_hashtag_df = troll[['user_key', 'tweet_hashtags']]\nuser_hashtag_df = user_hashtag_df.reset_index(drop = True)","metadata":{"_uuid":"f5746f0423d47a45a1916b91c38f7b93a8a5ce2f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets remove the rows where no hashtags were used\nrow_remove_mask = user_hashtag_df['tweet_hashtags'].map(lambda x: \"0\" in x)\n\n# Remove these rows from the user hashtag df\nuser_hashtag_df_clean = user_hashtag_df.loc[~row_remove_mask, :]\nuser_hashtag_df_clean.reset_index(drop = True, inplace = True)\nuser_hashtag_df_clean.head()","metadata":{"_uuid":"dfaf4082487f4edab35e8b657a33c23e84374435","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate out all hashtags used.\nall_hashtags = user_hashtag_df_clean['tweet_hashtags']","metadata":{"_uuid":"6605e9b5d61edde745f27192beb658bfec497f23","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so what I wanted was to create columns with the different hashtags used as the column names and the count of the number of times they appear in a tweet by a person as the value. The code below should have done that, however, I find that it eats up all my RAM and crashes the Kernel. This is because there are around 28k unique hashtags used in around 100,000 different tweets.","metadata":{"_uuid":"d2eb39423cbd6b8ccaecbee6b9d5e4f96c32fc03"}},{"cell_type":"code","source":"# count_dict = {}\n# count_df = pd.DataFrame()\n# for i in range(len(hashtag_counts.index.values)):\n#     count_hash = all_hashtags.map(lambda x: x.count(hashtag_counts.index.values[i]))\n#     count_dict[i] = count_hash\n#     if i == 5000:\n#         count_df = pd.DataFrame(count_dict)\n#         count_dict = {}\n#     elif i % 5000 == 0:\n#         count_df = pd.concat([count_df, pd.DataFrame(count_dict)])\n#         count_dict = {}\n#     else:\n#         next","metadata":{"_uuid":"33dc325b53f1885f318ab736d12c32b7eb82c2e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So my alternate strategy is to count the number of tweets with hashtags that a hashtag appears in and filter out those hashtags that are present in less than 50 tweets. Hopefully this should give me a less dramatic dataframe to work with.","metadata":{"_uuid":"939b169885773971127168062c8e23d85992f518"}},{"cell_type":"code","source":"# get hashtags that qualify - present in 50 or more tweets\nqualify_hashtags_mask = (hashtag_counts >= 50)\nqualify_hashtags = hashtag_counts[qualify_hashtags_mask]\n\n# remove the \"0\" hashtags\nqualify_hashtags = qualify_hashtags.drop(labels = \"0\")\nqualify_hashtags.head()","metadata":{"_uuid":"ca35dddfa13020603e6a2ef092d364f5c0bbe39e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***qualify_hashtags* now has 435 hashtags that are present in 50 or more different tweets.** Wow! Thats such a reduction from the 28000 unique hashtags that we originally had! \n\nMy hope is that now I should be able to count how many times those appear in the tweets and then make the dataframe I intended to above!","metadata":{"_uuid":"118546d6da2a8303f34c3945aedce85c7b7b2a52"}},{"cell_type":"code","source":"# lets count the number of times these qualified hashtags appear in the tweets with hashtags\ncount_dict = {}\n\nfor i in qualify_hashtags.index.values:\n    count_hash = all_hashtags.map(lambda x: x.count(i))\n    count_dict[i] = count_hash\n\n# create a dataframe from the hashtags and their counts in tweets\nhashtag_count_df = pd.DataFrame(count_dict)\n\n# concatenate this dataframe with the hashtag_count_df\nuser_hashtag_count_df = pd.concat([user_hashtag_df_clean, hashtag_count_df], axis = 1)","metadata":{"_uuid":"df379d24d60914f653000c8f6f3126c681774ade","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, for each user now we have the count of the qualifying hashtags in each of their tweets. Lets group this by the user and get a sum of the counts of hashtags used!","metadata":{"_uuid":"6e81223c9df7d0aaeb9f120fa057f17f78a26404"}},{"cell_type":"code","source":"# group by user_key and get the sum of times they have used a hashtag\nuser_hashtag_group = user_hashtag_count_df.groupby('user_key').agg('sum').reset_index()\nuser_hashtag_group.head()","metadata":{"_uuid":"87758bd125a940e49b9f43bbc96329411c0ec42f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In Part 2 of this analysis, we take a look at clusters of users based on hashtags used!**","metadata":{"_uuid":"1e4f7d590f33f4ef57cebb5ae6b0e66923c998ab"}},{"cell_type":"markdown","source":"## Lets look at user wise tweet text\nRecall that we had created a *tweet_text_only* column that contained just the tweet text devoid of any links, RT, mentions or hashtags! Lets get a user wise text dataframe!","metadata":{"_uuid":"96dda0b550035699de7376bc17cb8180c07d595f"}},{"cell_type":"code","source":"user_tweet_df = troll.loc[:, ['user_key', 'tweet_text_only']]\nuser_tweet_df.head()","metadata":{"_uuid":"2c36f9d59c787ad93dd5e438f01bc89cd96c7a59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Users!\nWe also have data on the users who write these tweets! What can we gather from this data?\nLet's just look at the data first.","metadata":{"_uuid":"50d5a0c5ca8c2ac097c86814deeab65a96726ef7"}},{"cell_type":"code","source":"users = pd.read_csv('../input/users.csv')\nusers.head(2)","metadata":{"_uuid":"4f9a0feb58156384a4e3382bdb76840bc882f9ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see information about *user_id*, *user_name*, *follower_count*, *language*, etc.","metadata":{"_uuid":"ba5bff9d4b7beae3d9c480f4f6dcb2341a5bec03"}},{"cell_type":"markdown","source":"## Where are users from and what language do they use?\nLets create a sankey plot (also called alluvial plot in R) to get a sense of which time zone are the users from and what language they speak!","metadata":{"_uuid":"04340c19c0cc4b80bd798e6f4561e35baaddb072"}},{"cell_type":"code","source":"# First we get a count of users from each time-zone and language combination!\nuser_loc_lang = users.groupby(['time_zone', 'lang'])['id'].agg('count').reset_index()\nuser_loc_lang.rename(columns = {'id':'user_count'}, inplace = True)\nuser_loc_lang.head(5)","metadata":{"_uuid":"8e18b592eab0261b44bd5d0d3c39b2ec1dbcf0af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a custom package installed within kaggle kernel\nfrom pySankey import sankey\nsankey.sankey(user_loc_lang['time_zone'],\n              user_loc_lang['lang'],\n              leftWeight = user_loc_lang['user_count'],\n              rightWeight = user_loc_lang['user_count'], \n              fontsize = 10)\nplt.title(\"User profile\")","metadata":{"_uuid":"ec45be2412d826082df29076f04dedae2444f8f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above chart shows the user profile in the troll tweet users. **English speaking** users come from **US, Canada & Arizona**. *Russian* speaking users come from *Moscow, Volgograd, Yerevan and Minsk*! All french speaking users are from Paris. This makes sense!","metadata":{"_uuid":"e0d520fff0c0c97e5f3275168db0524becb99308"}},{"cell_type":"markdown","source":"## When were these accounts created?\nThe *created_at* column in the **users** dataframe captures this information!","metadata":{"_uuid":"a4be72ca0f924f249f04670f3e566d8f56550cea"}},{"cell_type":"code","source":"# First we convert the created_at to datetime and then extract the date from this\nusers['created_at'] = pd.to_datetime(users['created_at'])\nusers['created_at_date'] = pd.to_datetime(users['created_at'].dt.date)\n\nusers['created_at_date'].head()","metadata":{"_uuid":"dd53f64a542ac87265b806cd45ca6757d1511bc7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_created = users.groupby('created_at_date')['id'].agg('count')\n\nplt.style.use('fivethirtyeight')\nuser_created.resample('W',kind = 'period').sum().\\\nplot(linestyle = '-', figsize = (10,8), linewidth = 1)\ntitle('Troll User Account Created')\nxlabel('Dates')\nylabel('Count of accounts created')","metadata":{"_uuid":"079eb9d974a382784f7bdcdf547b8ee465908433","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most troll accounts were created in the second half of 2013 or first half of 2014!","metadata":{"_uuid":"1573f6aec9ae6d855a80e386f2a48e6880e68c6f"}},{"cell_type":"markdown","source":"## Which user tweets the most?","metadata":{"_uuid":"2773bf33de0a5edf8e524035ffdb5ae034b0cf0b"}},{"cell_type":"code","source":"user_tweet_count = troll.groupby('user_id')['text'].agg('count').reset_index()\nuser_tweet_count.rename(columns = {'text':'Tweet_count'}, inplace = True)","metadata":{"_uuid":"45b19abbc4b6789ad78c7764dca5289b9e034b71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's merge this dataframe with the **users** data to get the associated *followers_count* and *name*.","metadata":{"_uuid":"d91c342282c0b1be411231637004637e0759da2b"}},{"cell_type":"code","source":"user_tweet_count_df = user_tweet_count.merge(users,\n                                      left_on = 'user_id',\n                                      right_on = 'id')\nuser_tweet_count_df.head(2)","metadata":{"_uuid":"de79028913e5a4bfe1a5c0fa691338bb81c591b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A tweet count of the top 10 tweeting users follows-","metadata":{"_uuid":"0afbfa0672207e6a66ec96143a2cdfb2f3cfae5a"}},{"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\nuser_tweet_count_df[['name', 'Tweet_count']].sort_values('Tweet_count', ascending = False)[:10].\\\nset_index('name').plot(kind = 'barh', figsize = (10,8))\ntitle('User Wise Tweet Count', fontsize = 15)\nxlabel('Tweet Count', fontsize = 13)\nylabel('User Name', fontsize = 13)","metadata":{"_uuid":"1205116daceac705942afce33f6f9cb466d4ab0c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Do a larger number of tweets mean higher number of followers?\nLets see if there is a linear correlation here!","metadata":{"_uuid":"d1501c99014c32106b7d0791e50070a1022f1815"}},{"cell_type":"code","source":"correl = user_tweet_count_df['Tweet_count'].corr(user_tweet_count_df['followers_count'])\nprint(\"{0:.2f}\".format(correl))","metadata":{"_uuid":"bb6fea0ff34d3362fdb0c829c85c26f7e5b09a8b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drawing a scatterplot of the tweet count with number of followers\nfig = plt.figure(figsize = (10,8))\nplt.style.use('seaborn-darkgrid')\nplt.scatter(user_tweet_count_df['Tweet_count'], \n        user_tweet_count_df['followers_count'],\n       marker = 'o',\n       alpha = 0.5)\nplt.title(\"Followers vs Number of Tweets\", fontsize = 15)\nplt.xlabel(\"Number of Tweets\", fontsize = 13)\nplt.ylabel(\"Follower Count\", fontsize = 13)\nplt.text(6000, 80000, s = \"Correlation is: {0:.2f}\".format(correl), fontsize = 15)","metadata":{"_uuid":"beeef25c29d8bcc5d3b20bb446bd78d962fa2793","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see from the chart that no such correlation exists!** Most users have very low tweet counts but their followers range from very few to numerous!","metadata":{"_uuid":"28e5fdd738046eeca4429fc34c727a228aa23893"}},{"cell_type":"markdown","source":"## What are the languages with which users registered?\nThe table below shows that english is the most common language, followed by Russian and German!","metadata":{"_uuid":"1cde741b32494d94f36fb3f4de6cac548d997cc3"}},{"cell_type":"code","source":"user_tweet_count_df['lang'].value_counts()","metadata":{"_uuid":"27fd46e7cf5544ab8bb1c3f6198290789997622e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Which users had the most influence?\nWe see that although there are only 90 users with language 'ru', there are 4 of them in the top 10 most followed users!","metadata":{"_uuid":"6dbc8e7fc3ca3ba09c1aab16247d6675a33bd8f4"}},{"cell_type":"code","source":"user_tweet_count_df[['name', 'lang', 'followers_count']].sort_values('followers_count', \n                                                               ascending = False)[:10]","metadata":{"_uuid":"a1429cdbd9891c54f1b59dc2fd819d40fc9f582e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets write out these files as datasets!\nuser_mention_clean_df.to_csv('User_Mentions.csv')\nuser_hashtag_group.to_csv('User_Hashtags.csv')\nuser_tweet_df.to_csv('User_Tweets.csv')","metadata":{"_uuid":"30e007ea357f573d25f18aaad7e35843236655c9","execution":{"iopub.status.busy":"2023-12-16T12:54:51.718588Z","iopub.execute_input":"2023-12-16T12:54:51.718891Z","iopub.status.idle":"2023-12-16T12:54:51.916614Z","shell.execute_reply.started":"2023-12-16T12:54:51.718842Z","shell.execute_reply":"2023-12-16T12:54:51.914605Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-88649c5a9283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lets write out these files as datasets!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muser_mention_clean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Mentions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muser_hashtag_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Hashtags.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muser_tweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User_Tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'user_mention_clean_df' is not defined"],"ename":"NameError","evalue":"name 'user_mention_clean_df' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}